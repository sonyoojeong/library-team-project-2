{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9177a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44cb97af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h2 class=\"gd_name\">진짜 쓰는 실무 엑셀</h2>]\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.yes24.com/Product/Goods/106711883'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "tag_name = '#yDetailTopWrap > div.topColRgt > div.gd_infoTop > div > h2'\n",
    "books = soup.select(tag_name)\n",
    "print(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0733864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "진짜 쓰는 실무 엑셀\n"
     ]
    }
   ],
   "source": [
    "name = books[0].text\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933af48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수집 함수 정의\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "def yes24DataReader(CategoryNumber, year, month):\n",
    "\n",
    "    root_url = 'http://www.yes24.com'\n",
    "\n",
    "    url_1 = 'http://www.yes24.com/24/category/bestseller?CategoryNumber='\n",
    "    url_2 = '&sumgb=09&year='\n",
    "    url_3 = '&month='\n",
    "    url_4 = '&PageNumber='\n",
    "    url_set = url_1 + CategoryNumber + url_2 + year + url_3 + month + url_4\n",
    "\n",
    "    book_list=[]\n",
    "\n",
    "    # 월 별 조회 시 최대 50쪽이지만, 간단한 설명을 위해 2쪽까지만 수집\n",
    "    for i in range(1, 3):\n",
    "\n",
    "        url = url_set + str(i)\n",
    "\n",
    "        res = requests.post(url)\n",
    "        soup = BeautifulSoup(res.text, 'html5lib')\n",
    "        tag = '#category_layout > tbody > tr > td.goodsTxtInfo > p:nth-child(1) > a:nth-child(1)'\n",
    "        books = soup.select(tag)\n",
    "\n",
    "        # 수집 중인 페이지 번호 출력\n",
    "        print('# Page', i)\n",
    "\n",
    "        # 개별 도서 정보 수집\n",
    "        for book in books:\n",
    "\n",
    "            sub_url = root_url + book.attrs['href']\n",
    "            sub_res = requests.post(sub_url)\n",
    "            sub_soup = BeautifulSoup(sub_res.text, 'html5lib')\n",
    "\n",
    "            tag_name = '#yDetailTopWrap > div.topColRgt > div.gd_infoTop > div > h2'\n",
    "            tag_author = '#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_auth > a'\n",
    "            tag_author2 = '#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_auth'\n",
    "            tag_publisher = '#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_pub > a'\n",
    "            tag_date = '#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_date'\n",
    "         \n",
    "            \n",
    "            tag_price = '#yDetailTopWrap > div.topColRgt > div.gd_infoBot > div.gd_infoTbArea > div:nth-child(3) > table > tbody > tr:nth-child(2) > td > span > em'\n",
    "            tag_price2 = '#yDetailTopWrap > div.topColRgt > div.gd_infoBot > div.gd_infoTbArea > div:nth-child(4) > table > tbody > tr:nth-child(2) > td > span > em'\n",
    "\n",
    "            tag_page = '#infoset_specific > div.infoSetCont_wrap > div > table > tbody > tr:nth-child(2) > td'\n",
    "            tag_weight = '#infoset_specific > div.infoSetCont_wrap > div > table > tbody > tr:nth-child(2) > td'\n",
    "            tag_hor = '#infoset_specific > div.infoSetCont_wrap > div > table > tbody > tr:nth-child(2) > td'\n",
    "            tag_ver = '#infoset_specific > div.infoSetCont_wrap > div > table > tbody > tr:nth-child(2) > td'\n",
    "            tag_width = '#infoset_specific > div.infoSetCont_wrap > div > table > tbody > tr:nth-child(2) > td'\n",
    "            tag_isbn13 = '#infoset_specific > div.infoSetCont_wrap > div > table > tbody > tr:nth-child(3) > td'\n",
    "            tag_description = '#infoset_introduce > div.infoSetCont_wrap > div.infoWrap_txt > div > textarea'\n",
    "         \n",
    "\n",
    "\n",
    "            # 기본적인 예외처리를 통한 데이터 수집\n",
    "            name = sub_soup.select(tag_name)[0].text\n",
    "\n",
    "            try:\n",
    "                author = sub_soup.select(tag_author)[0].text\n",
    "            except:\n",
    "                author = sub_soup.select(tag_author2)[0].text.strip('\\n').strip().replace(' 저','')\n",
    "\n",
    "\n",
    "            publisher = sub_soup.select(tag_publisher)[0].text\n",
    "            date = sub_soup.select(tag_date)[0].text.replace('년 ','-').replace('월 ','-').replace('일','')\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "            try:\n",
    "                price = sub_soup.select(tag_price)[0].text.replace(',','')\n",
    "            except:\n",
    "                try:\n",
    "                    price = sub_soup.select(tag_price2)[0].text.replace(',','')\n",
    "                except:\n",
    "                    price = ''\n",
    "\n",
    "            page = sub_soup.select(tag_page)[0].text\n",
    "            if '쪽' in page:\n",
    "                if '확인' in page:\n",
    "                    page = ''\n",
    "                else :\n",
    "                    page = page.split('|')[0].strip().replace('쪽','')\n",
    "            else :\n",
    "                page = ''\n",
    "\n",
    "            weight = sub_soup.select(tag_weight)[0].text\n",
    "            if 'g' in weight:\n",
    "                weight = weight[:weight.find('g')].split('|')[1].strip()\n",
    "            else :\n",
    "                weight = ''\n",
    "\n",
    "            hvw = sub_soup.select(tag_hor)[0].text\n",
    "            if 'mm' in hvw:\n",
    "\n",
    "                if hvw.split('|')[-1].strip().count('*')==2:\n",
    "\n",
    "                    hor = hvw.split('|')[-1].strip().split('*')[0]\n",
    "                    ver = hvw.split('|')[-1].strip().split('*')[1]\n",
    "                    width = hvw.split('|')[-1].strip().split('*')[2].replace('mm','')\n",
    "\n",
    "                elif hvw.split('|')[-1].strip().count('*')==1:\n",
    "\n",
    "                    hor = hvw.split('|')[-1].strip().split('*')[0]\n",
    "                    ver = hvw.split('|')[-1].strip().split('*')[1].replace('mm','')\n",
    "                    width = ''\n",
    "\n",
    "            else :\n",
    "                hor = ''\n",
    "                ver = ''\n",
    "                width = ''\n",
    "\n",
    "            try :\n",
    "                isbn13 = sub_soup.select(tag_isbn13)[0].text\n",
    "                if '확인' in isbn13:\n",
    "                    isbn13 = ''\n",
    "                else :\n",
    "                    isbn13 = sub_soup.select(tag_isbn13)[0].text\n",
    "            except :\n",
    "                isbn13 = ''\n",
    "                \n",
    "            try:\n",
    "                description = sub_soup.select(tag_description)[0].text\n",
    "                description = description.replace('<br/>' ,'')  # <br/> 태그 제거\n",
    "                cleanr = re.compile('<.*?>')\n",
    "                description = re.sub(cleanr, '', description)\n",
    "            except IndexError:\n",
    "                description = \"\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            book_list.append([name, author, publisher, date,\n",
    "                               price, page,\n",
    "                              weight, hor, ver, width, isbn13,description ])\n",
    "\n",
    "            print('=========>', name)\n",
    "\n",
    "    # 데이터프레임 컬럼명 지정\n",
    "    colList = ['name',  'author', 'publisher', 'date',\n",
    "                'price', 'page',\n",
    "               'weight', 'hor', 'ver', 'width', 'isbn13','description']\n",
    "\n",
    "    # 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(np.array(book_list), columns=colList)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496dd081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "# Year 2023\n",
      "==================================================\n",
      "==================================================\n",
      "# Month 1\n",
      "==================================================\n",
      "# Page 1\n",
      "=========> 모든 것은 기본에서 시작한다\n",
      "=========> 물고기는 존재하지 않는다\n",
      "=========> 너의 하루가 따숩길 바라\n",
      "=========> 인생의 역사\n",
      "=========> 기분이 태도가 되지 말자\n",
      "=========> 불안\n",
      "=========> 잘될 수밖에 없는 너에게\n",
      "=========> 천문학자는 별을 보지 않는다\n",
      "=========> 보이지 않는 곳에서 애쓰고 있는 너에게\n",
      "=========> 축구를 하며 생각한 것들 (리커버 에디션)\n",
      "=========> 빅터 프랭클의 죽음의 수용소에서\n",
      "=========> 기분을 관리하면 인생이 관리된다\n",
      "=========> 너의 안부\n",
      "=========> 인듀어런스\n",
      "=========> 생에 감사해\n",
      "=========> 사람을 얻는 지혜 \n",
      "=========> Hoy \n",
      "=========> 망그러진 만화\n",
      "=========> 잘했고 잘하고 있고 잘 될 것이다 (스페셜 리미티드 에디션)\n",
      "=========> 내 안의 어린아이에게 세트\n",
      "# Page 2\n",
      "=========> 100세 철학자의 행복론\n",
      "=========> 2023년 문재인 일력\n",
      "=========> 여행의 이유\n",
      "=========> 착한 아이 버리기 \n",
      "=========> 나는 나로 살기로 했다\n",
      "=========> 나는 당신이 행복했으면 좋겠습니다\n",
      "=========> 내일, 내가 다시 좋아지고 싶어\n",
      "=========> 아주 오랜만에 행복하다는 느낌\n",
      "=========> 새는 날아가면서 뒤돌아보지 않는다\n",
      "=========> H마트에서 울다\n",
      "=========> 나는 나를 응원합니다\n",
      "=========> 어떤 고독은 외롭지 않다\n",
      "=========> 하루 한 문장, 고전 명작 일력 \n",
      "=========> 빅터 프랭클의 죽음의 수용소에서\n",
      "=========> 상대적이며 절대적인 고양이 백과사전\n",
      "=========> 작은 별이지만 빛나고 있어 (20만 부 기념 한정판 에디션)\n",
      "=========> 저 별은 모두 당신을 위해 빛나고 있다\n",
      "=========> 애쓰지 않고 편안하게 (20만 부 기념 에디션)\n",
      "=========> 나를 위한 노래\n",
      "=========> 내가 같이 뛰어내려 줄게\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 역사 카테고리 번호 : 001001010\n",
    "# IT 카테고리 번호 : 001001003\n",
    "# 여행 카테고리 번호 : 001001009\n",
    "# 한국소설 카테고리 번호 : 001001046001\n",
    "# 외국소설 카테고리 번호 : 001001046002\n",
    "# 사회정치 카테고리 번호 : 001001022\n",
    "# 경제경영 카테고리 번호 : 001001025\n",
    "# 예술 카테고리 번호 : 001001007\n",
    "# 어린이 카테고리 번호 : 001001016\n",
    "# 에세이 카테고리 번호 : 001001047\n",
    "CategoryNum='001001047'\n",
    "\n",
    "\n",
    "# 2019년도\n",
    "for year in range(2023, 2024):\n",
    "    print('='*50)\n",
    "    print('# Year', year)\n",
    "    print('='*50)\n",
    "\n",
    "    # 1월\n",
    "    for month in range(1, 2):\n",
    "        print('='*50)\n",
    "        print('# Month', month)\n",
    "        print('='*50)\n",
    "\n",
    "        # 월 별 데이터 수집\n",
    "        df = yes24DataReader(CategoryNum, str(year), str(month))\n",
    "\n",
    "        # 월 별로 수집된 데이터를 CSV 형식 파일로 저장\n",
    "        df.to_csv(str(year)+'_'+str(month)+'_'+str(CategoryNum)+'.csv', index=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1e1c195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "# Year 2023\n",
      "==================================================\n",
      "==================================================\n",
      "# Month 1\n",
      "==================================================\n",
      "==================================================\n",
      "# Month 2\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.request import urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def yes24DataReader(CategoryNum, year, month):\n",
    "    url = f'http://www.yes24.com/24/category/bestseller?CategoryNumber={CategoryNum}&sumgb=06&year={year}&month={month}'\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    books = soup.select('ol > li')\n",
    "    book_list = []\n",
    "\n",
    "    for book in books:\n",
    "        # 도서 제목 수집\n",
    "        title = book.select('a')[0]['title']\n",
    "\n",
    "        # 도서 이미지 링크 수집\n",
    "        img_src = book.select('a > img')[0]['src']\n",
    "\n",
    "        # 도서 설명 페이지로 이동\n",
    "        description_link = book.select('a')[0]['href']\n",
    "        sub_res = requests.get(description_link)\n",
    "        sub_soup = BeautifulSoup(sub_res.text, 'lxml')\n",
    "\n",
    "        # 도서 설명 수집\n",
    "        try:\n",
    "            description = sub_soup.select('div.infoWrap_txt')[0].text.strip()\n",
    "            description = re.sub('\\s+', ' ', description) # 공백 제거\n",
    "            description = description.replace('<br/>' ,'')  # <br/> 태그 제거\n",
    "        except IndexError:\n",
    "            description = \"\"\n",
    "\n",
    "        # 도서 정보를 딕셔너리에 저장\n",
    "        book_dict = {\n",
    "            'Title': title,\n",
    "            'Image Link': img_src,\n",
    "            'Description': description\n",
    "        }\n",
    "\n",
    "        # 도서 이미지 저장\n",
    "        image_save(img_src, title)\n",
    "\n",
    "        # 딕셔너리를 리스트에 추가\n",
    "        book_list.append(book_dict)\n",
    "\n",
    "    # 리스트를 데이터프레임으로 변환\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(book_list)\n",
    "\n",
    "    return df\n",
    "\n",
    "def image_save(src, title):\n",
    "    savename = '/Users/woongcor/images/' + title + \".jpg\"\n",
    "    urlretrieve(src, savename)\n",
    "    print(title, '이미지 저장됨')\n",
    "\n",
    "# 역사 카테고리 번호 : 001001010\n",
    "CategoryNum='001001003'\n",
    "\n",
    "# 2019년도\n",
    "for year in range(2023, 2024):\n",
    "    print('='*50)\n",
    "    print('# Year', year)\n",
    "    print('='*50)\n",
    "\n",
    "    # 9월\n",
    "    for month in range(1, 3):\n",
    "        print('='*50)\n",
    "        print('# Month', month)\n",
    "        print('='*50)\n",
    "\n",
    "        # 월 별 데이터 수집\n",
    "        df = yes24DataReader(CategoryNum, str(year), str(month))\n",
    "\n",
    "        # 월 별로 수집된 데이터를 CSV 형식 파일로 저장\n",
    "        df.to_csv(str(year)+'_'+str(month)+'_'+str(CategoryNum)+'.csv', index=False, encoding='UTF-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa83960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd1cc32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "# Year 2023\n",
      "==================================================\n",
      "==================================================\n",
      "# Month 1\n",
      "==================================================\n",
      "# Page 1\n",
      "=========> 진짜 쓰는 실무 엑셀\n",
      "=========> 혼자 공부하는 파이썬\n",
      "=========> 제품의 탄생\n",
      "=========> 혼자 공부하는 머신러닝+딥러닝\n",
      "=========> 실무에 바로 쓰는 일잘러의 보고서 작성법\n",
      "=========> 퇴근 후 스마트스토어로 투잡하기\n",
      "=========> Do it! 점프 투 파이썬\n",
      "=========> 만들면서 배우는 파이썬과 40개의 작품들\n",
      "=========> 회사에서 바로 통하는 실무 엑셀+파워포인트+워드&한글 \n",
      "=========> 비전공자도 이해할 수 있는 AI 지식\n",
      "=========> 혼자 공부하는 컴퓨터 구조+운영체제\n",
      "=========> 회사는 엑셀을 가르쳐주지 않아요\n",
      "=========> 비전공자를 위한 이해할 수 있는 IT 지식 \n",
      "=========> 개발자 원칙\n",
      "=========> 유튜브 영상 편집을 위한 프리미어 프로\n",
      "=========> Clean Code 클린 코드\n",
      "=========> NestJS로 배우는 백엔드 프로그래밍\n",
      "=========> 업무와 일상을 정리하는 새로운 방법 노션 Notion\n",
      "=========> 코딩 자율학습 나도코딩의 C 언어 입문\n",
      "=========> 구글 엔지니어는 이렇게 일한다\n",
      "# Page 2\n",
      "=========> 혼자 해도 프로 작가처럼 잘 그리는 아이패드 드로잉 with 프로크리에이트\n",
      "=========> 혼자 공부하는 C 언어\n",
      "=========> 맛있는 디자인 포토샵&일러스트레이터 CC 2022\n",
      "=========> Java의 정석\n",
      "=========> 1일 1로그 100일 완성 IT 지식\n",
      "=========> 국내 최다 이모티콘 승인 작가 씨엠제이가 알려주는 승인율 99.9% 이모티콘 만들기\n",
      "=========> 된다! 네이버 블로그 상위 노출\n",
      "=========> 면접을 위한 CS 전공지식 노트\n",
      "=========> Do it! HTML+CSS+자바스크립트 웹 표준의 정석\n",
      "=========> 혼자 공부하는 SQL\n",
      "=========> 모던 자바스크립트 Deep Dive\n",
      "=========> 클린 아키텍처\n",
      "=========> 실무에서 바로 쓰는 파워포인트 디자인\n",
      "=========> 네이버쇼핑 스마트스토어로 상위노출 하라\n",
      "=========> 일잘러의 비밀, 엑셀 대신 파이썬으로 업무 자동화하기\n",
      "=========> IT 트렌드 2023\n",
      "=========> 이것이 취업을 위한 코딩 테스트다 with 파이썬 \n",
      "=========> 혼자 공부하는 얄팍한 코딩 지식\n",
      "=========> 트랜스포머를 활용한 자연어 처리\n",
      "=========> 된다! 실무 엑셀 파워포인트 워드&한글\n",
      "==================================================\n",
      "# Month 2\n",
      "==================================================\n",
      "# Page 1\n",
      "=========> 진짜 쓰는 실무 엑셀\n",
      "=========> 혼자 공부하는 파이썬\n",
      "=========> Do it! 점프 투 파이썬\n",
      "=========> 퇴근 후 스마트스토어로 투잡하기\n",
      "=========> 제품의 탄생\n",
      "=========> 실무에 바로 쓰는 일잘러의 보고서 작성법\n",
      "=========> 혼자 공부하는 머신러닝+딥러닝\n",
      "=========> 혼자 공부하는 컴퓨터 구조+운영체제\n",
      "=========> 회사에서 바로 통하는 실무 엑셀+파워포인트+워드&한글 \n",
      "=========> 비전공자를 위한 이해할 수 있는 IT 지식 \n",
      "=========> 혼자 공부하는 C 언어\n",
      "=========> 면접을 위한 CS 전공지식 노트\n",
      "=========> 유튜브 영상 편집을 위한 프리미어 프로\n",
      "=========> Java의 정석\n",
      "=========> Must Have 코드팩토리의 플러터 프로그래밍\n",
      "=========> 된다! 네이버 블로그 상위 노출\n",
      "=========> 만들면서 배우는 파이썬과 40개의 작품들\n",
      "=========> 비전공자도 이해할 수 있는 AI 지식\n",
      "=========> IT 5분 잡학사전\n",
      "=========> Clean Code 클린 코드\n",
      "# Page 2\n",
      "=========> 혼자 공부하는 데이터 분석 with 파이썬\n",
      "=========> 모던 자바스크립트 Deep Dive\n",
      "=========> AI 2041\n",
      "=========> 회사는 엑셀을 가르쳐주지 않아요\n",
      "=========> 혼자 해도 프로 작가처럼 잘 그리는 아이패드 드로잉 with 프로크리에이트\n",
      "=========> 업무와 일상을 정리하는 새로운 방법 노션 Notion\n",
      "=========> 국내 최다 이모티콘 승인 작가 씨엠제이가 알려주는 승인율 99.9% 이모티콘 만들기\n",
      "=========> 데이터로 말한다! 퍼포먼스 마케팅\n",
      "=========> NestJS로 배우는 백엔드 프로그래밍\n",
      "=========> 개발자 원칙\n",
      "=========> 코딩 자율학습 나도코딩의 C 언어 입문\n",
      "=========> 1일 1로그 100일 완성 IT 지식\n",
      "=========> IT 트렌드 2023\n",
      "=========> Do it! HTML+CSS+자바스크립트 웹 표준의 정석\n",
      "=========> 혼자 공부하는 SQL\n",
      "=========> 나는 매일 인스타그램으로 돈 번다\n",
      "=========> 인스타그램&네이버 블로그로 매출이 올라가는 입소문 만들기\n",
      "=========> 이것이 취업을 위한 코딩 테스트다 with 파이썬 \n",
      "=========> 윤성우의 열혈 C 프로그래밍\n",
      "=========> 된다! 실무 엑셀 파워포인트 워드&한글\n"
     ]
    }
   ],
   "source": [
    "# 지피티 센세\n",
    "from bs4 import BeautifulSoup \n",
    "import urllib\n",
    "import re\n",
    "\n",
    "# 역사 카테고리 번호 : 001001010\n",
    "CategoryNum='001001003'\n",
    "\n",
    "# 검색할 사이트\n",
    "root_url = \"http://www.yes24.com/24/category/bestseller?CategoryNumber=\" + CategoryNum + \"&sumgb=06\"\n",
    "\n",
    "# HTML 헤더\n",
    "head = {'User-Agent' : \"Magic Browser\"}\n",
    "\n",
    "# 2019년도\n",
    "for year in range(2023, 2024):\n",
    "    print('='*50)\n",
    "    print('# Year', year)\n",
    "    print('='*50)\n",
    "\n",
    "    # 1월부터 2월까지\n",
    "    for month in range(1, 3):\n",
    "        print('='*50)\n",
    "        print('# Month', month)\n",
    "        print('='*50)\n",
    "\n",
    "        # URL 조합하기\n",
    "        url = root_url + '&year=' + str(year) + '&month=' + str(month)\n",
    "\n",
    "        # 요청보내기\n",
    "        request = urllib.request.Request(url, headers = head) \n",
    "\n",
    "        # 응답 받기\n",
    "        response = urllib.request.urlopen(request)\n",
    "\n",
    "        # 응답을 Beautifulsoup 패키징화하기\n",
    "        soup = BeautifulSoup(response.read().decode('euc-kr','replace'))\n",
    "\n",
    "        # 월 별 데이터 수집\n",
    "        df = yes24DataReader(CategoryNum, str(year), str(month))\n",
    "        \n",
    "        # 월 별로 수집된 데이터를 CSV 형식 파일로 저장\n",
    "        df.to_csv(str(year)+'_'+str(month)+'_'+str(CategoryNum)+'.csv', index=False, encoding='UTF-8') \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60501c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import urllib\n",
    "\n",
    "# 검색할 사이트\n",
    "root_url = \"http://www.yes24.com/24/category/bestseller?CategoryNumber=001001003&sumgb=09\"\n",
    "\n",
    "# 원하는 연도와 월의 범위 설정\n",
    "years = range(2023, 2024)\n",
    "months = range(1, 6)\n",
    "\n",
    "# URL 조합 및 스크래핑 수행\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        url = root_url + f\"&year={year}&month={month}\"\n",
    "        \n",
    "        # HTML 헤더\n",
    "        head = {'User-Agent' : \"Magic Browser\"}\n",
    "\n",
    "        # 요청보내기\n",
    "        request = urllib.request.Request(url, headers=head) \n",
    "\n",
    "        # 응답 받기\n",
    "        response = urllib.request.urlopen(request)\n",
    "\n",
    "        # 응답을 Beautifulsoup 패키징화하기\n",
    "        soup = BeautifulSoup(response.read().decode('euc-kr','replace'))\n",
    "\n",
    "        # 이후에 웹 스크래핑 코드 작성\n",
    "        # 예를 들어, 베스트셀러 도서 제목 추출\n",
    "        book_titles = soup.select('ol li p.copy > a')\n",
    "        for title in book_titles:\n",
    "            print(title.text.strip())\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48a7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # 시스템 함수\n",
    "\n",
    "# 이미지를 저장할 디렉토리\n",
    "directory_path = \"book_image\"\n",
    "\n",
    "# 디렉토리가 존재하지 않을 시 자동 생성\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ead3927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://image.yes24.com/goods/117936860/S\n",
      "진짜 챗GPT 활용법\n",
      "http://image.yes24.com/goods/106711883/S\n",
      "진짜 쓰는 실무 엑셀\n",
      "http://image.yes24.com/goods/117937260/S\n",
      "챗GPT와 업무자동화\n",
      "http://image.yes24.com/goods/117603800/S\n",
      "챗GPT와 글쓰기\n",
      "http://image.yes24.com/goods/109625396/S\n",
      "혼자 공부하는 파이썬\n",
      "http://image.yes24.com/goods/107490270/S\n",
      "만들면서 배우는 파이썬과 40개의 작품들\n",
      "http://image.yes24.com/goods/112941801/S\n",
      "회사는 엑셀을 가르쳐주지 않아요\n",
      "http://image.yes24.com/goods/74419916/S\n",
      "Do it! 점프 투 파이썬\n",
      "http://image.yes24.com/goods/118266847/S\n",
      "Docs for Developers 기술 문서 작성 완벽 가이드\n",
      "http://image.yes24.com/goods/96024871/S\n",
      "혼자 공부하는 머신러닝+딥러닝\n",
      "http://image.yes24.com/goods/111378840/S\n",
      "혼자 공부하는 컴퓨터 구조+운영체제\n",
      "http://image.yes24.com/goods/93068762/S\n",
      "퇴근 후 스마트스토어로 투잡하기\n",
      "http://image.yes24.com/goods/117024821/S\n",
      "맛있는 디자인 포토샵&일러스트레이터 CC 2023\n",
      "http://image.yes24.com/goods/109662792/S\n",
      "회사에서 바로 통하는 실무 엑셀+파워포인트+워드&한글 \n",
      "http://image.yes24.com/goods/95162490/S\n",
      "실무에 바로 쓰는 일잘러의 보고서 작성법\n",
      "http://image.yes24.com/goods/117971215/S\n",
      "개발자를 위한 챗GPT 활용법\n",
      "http://image.yes24.com/goods/11681152/S\n",
      "Clean Code 클린 코드\n",
      "http://image.yes24.com/goods/91165789/S\n",
      "비전공자를 위한 이해할 수 있는 IT 지식 \n",
      "http://image.yes24.com/goods/117843203/S\n",
      "네이버 스마트스토어 시작하기\n",
      "http://image.yes24.com/goods/108887922/S\n",
      "면접을 위한 CS 전공지식 노트\n"
     ]
    }
   ],
   "source": [
    "# 도서 이미지를 포함하고 있는 태그\n",
    "contents = soup.find_all('div', {'class':'goodsImgW'})\n",
    "\n",
    "for content in contents:\n",
    "    \n",
    "    # 각 도서를 의미하는 <a> 태그\n",
    "    book_image = content.find_all('a')\n",
    "\n",
    "    # 태그안에 있는 모든 이미지\n",
    "    for i in range(0, len(book_image)):\n",
    "        if i % 2 ==0:\n",
    "            # 이미지 URL\n",
    "            image_url = book_image[i].find_all('img')[0].get(\"src\")\n",
    "            print(image_url)\n",
    "\n",
    "            # 이미지 이름\n",
    "            image_name = book_image[i].find_all('img')[0].get(\"alt\")\n",
    "            print(image_name)\n",
    "\n",
    "            urllib.request.urlretrieve(image_url, directory_path+\"/\"+image_name+\".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4153ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
